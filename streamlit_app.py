import streamlit as st
import os
import tempfile
from pathlib import Path
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.document_loaders import (
    TextLoader, 
    PyPDFLoader, 
    Docx2txtLoader,
    CSVLoader
)
import pandas as pd
from typing import List, Optional

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹", 
    page_icon="ğŸ¤–", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        padding: 20px 0;
        background: linear-gradient(90deg, #f0f2f6, #ffffff);
        border-radius: 10px;
        margin-bottom: 30px;
    }
    .upload-section {
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 20px;
    }
    .status-box {
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
    }
    .info-box {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        color: #0c5460;
    }
</style>
""", unsafe_allow_html=True)

class EnhancedRAGSystem:
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.memory = ConversationBufferWindowMemory(
            k=10,  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
            return_messages=True,
            memory_key="chat_history"
        )
        
    def initialize_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("âŒ è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
            st.stop()
        
        if self.embeddings is None:
            self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        return self.embeddings
    
    def load_document(self, file_path: str, file_type: str) -> List[Document]:
        """æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£"""
        try:
            if file_type.lower() == 'txt' or file_type.lower() == 'md':
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_type.lower() == 'pdf':
                loader = PyPDFLoader(file_path)
            elif file_type.lower() in ['docx', 'doc']:
                loader = Docx2txtLoader(file_path)
            elif file_type.lower() == 'csv':
                loader = CSVLoader(file_path)
            else:
                # é»˜è®¤å°è¯•æ–‡æœ¬åŠ è½½
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return [Document(page_content=content, metadata={"source": file_path})]
            
            return loader.load()
        except Exception as e:
            st.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ‡åˆ†æ–‡æ¡£"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
            separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
        )
        return text_splitter.split_documents(documents)
    
    def create_or_update_vectorstore(self, documents: List[Document]):
        """åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨"""
        if not documents:
            return
        
        embeddings = self.initialize_embeddings()
        
        if self.vectorstore is None:
            # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
            self.vectorstore = FAISS.from_documents(documents, embeddings)
            st.success(f"âœ… åˆ›å»ºäº†æ–°çš„çŸ¥è¯†åº“ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        else:
            # å¢é‡æ›´æ–°ç°æœ‰å‘é‡å­˜å‚¨
            new_vectorstore = FAISS.from_documents(documents, embeddings)
            self.vectorstore.merge_from(new_vectorstore)
            st.success(f"âœ… çŸ¥è¯†åº“å·²æ›´æ–°ï¼Œæ–°å¢ {len(documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    
    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨"""
        if self.vectorstore is None:
            return None
        return self.vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}  # è¿”å›æœ€ç›¸å…³çš„3ä¸ªç‰‡æ®µ
        )
    
    def build_qa_chain(self):
        """æ„å»ºé—®ç­”é“¾"""
        retriever = self.get_retriever()
        if retriever is None:
            return None
        
        llm = ChatOpenAI(
            model_name="gpt-4o", 
            temperature=0.1, 
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # æ”¹è¿›çš„promptæ¨¡æ¿ï¼Œæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†å’Œè‡ªèº«çŸ¥è¯†å›ç­”
        system_template = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹è§„åˆ™å›ç­”é—®é¢˜ï¼š

1. é¦–å…ˆæŸ¥çœ‹æä¾›çš„çŸ¥è¯†åº“å†…å®¹ï¼Œå¦‚æœèƒ½ä»ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºè¿™äº›ä¿¡æ¯å›ç­”
2. å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç»“åˆä½ è‡ªèº«çš„çŸ¥è¯†å’Œç»éªŒæ¥å›ç­”é—®é¢˜
3. è¯·ç»“åˆå¯¹è¯å†å²æ¥ç†è§£ç”¨æˆ·çš„é—®é¢˜ï¼Œä¿æŒå¯¹è¯çš„è¿è´¯æ€§
4. å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ä¸”å‹å¥½

çŸ¥è¯†åº“å†…å®¹ï¼š
{context}

å¯¹è¯å†å²ï¼š
{chat_history}

è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä½¿ç”¨äº†çŸ¥è¯†åº“ä¿¡æ¯ï¼Œè¯·è¯´æ˜ï¼›å¦‚æœä½¿ç”¨äº†è‡ªèº«çŸ¥è¯†ï¼Œä¹Ÿè¯·é€‚å½“è¯´æ˜ã€‚"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_template),
            ("human", "{question}")
        ])
        
        def format_chat_history(messages):
            """æ ¼å¼åŒ–èŠå¤©å†å²"""
            if not messages:
                return "æš‚æ— å†å²å¯¹è¯"
            
            formatted = []
            for msg in messages[-6:]:  # åªå–æœ€è¿‘6æ¡æ¶ˆæ¯
                if hasattr(msg, 'content'):
                    role = "ç”¨æˆ·" if msg.type == "human" else "åŠ©æ‰‹"
                    formatted.append(f"{role}: {msg.content}")
            return "\n".join(formatted)
        
        def get_relevant_docs(question):
            """è·å–ç›¸å…³æ–‡æ¡£"""
            if retriever:
                docs = retriever.get_relevant_documents(question)
                return "\n\n".join([doc.page_content for doc in docs]) if docs else "çŸ¥è¯†åº“ä¸­æš‚æ— ç›¸å…³ä¿¡æ¯"
            return "çŸ¥è¯†åº“ä¸ºç©º"
        
        chain = (
            {
                "context": lambda x: get_relevant_docs(x["question"]),
                "chat_history": lambda x: format_chat_history(self.memory.chat_memory.messages),
                "question": lambda x: x["question"]
            }
            | prompt
            | llm
            | StrOutputParser()
        )
        
        return chain
    
    def add_to_memory(self, question: str, answer: str):
        """æ·»åŠ åˆ°å¯¹è¯è®°å¿†"""
        self.memory.chat_memory.add_user_message(question)
        self.memory.chat_memory.add_ai_message(answer)

def main():
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = EnhancedRAGSystem()
    
    # åˆå§‹åŒ–èŠå¤©è®°å½•
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # ä¸»æ ‡é¢˜
    st.markdown('<div class="main-header">ğŸ¤– æ™ºèƒ½çŸ¥è¯†é—®ç­”åŠ©æ‰‹</div>', unsafe_allow_html=True)
    
    # ä¾§è¾¹æ  - çŸ¥è¯†åº“ç®¡ç†
    with st.sidebar:
        st.header("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")
        
        # æ˜¾ç¤ºå½“å‰çŸ¥è¯†åº“çŠ¶æ€
        rag_system = st.session_state.rag_system
        if rag_system.vectorstore is not None:
            doc_count = rag_system.vectorstore.index.ntotal
            st.markdown(f'<div class="status-box success-box">âœ… çŸ¥è¯†åº“å·²å‡†å¤‡å°±ç»ª<br>åŒ…å« {doc_count} ä¸ªæ–‡æ¡£ç‰‡æ®µ</div>', unsafe_allow_html=True)
        else:
            st.markdown('<div class="status-box warning-box">âš ï¸ çŸ¥è¯†åº“ä¸ºç©º<br>è¯·ä¸Šä¼ æ–‡æ¡£æ¥æ„å»ºçŸ¥è¯†åº“</div>', unsafe_allow_html=True)
        
        st.markdown("---")
        
        # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
        st.subheader("ğŸ“ ä¸Šä¼ æ–‡æ¡£")
        uploaded_files = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶", 
            type=['txt', 'md', 'pdf', 'docx', 'doc', 'csv'],
            accept_multiple_files=True,
            help="æ”¯æŒå¤šç§æ ¼å¼ï¼šTXT, MD, PDF, DOCX, CSV"
        )
        
        if uploaded_files:
            if st.button("ğŸš€ æ·»åŠ åˆ°çŸ¥è¯†åº“", use_container_width=True):
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                    all_documents = []
                    
                    for uploaded_file in uploaded_files:
                        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                            tmp_file.write(uploaded_file.getvalue())
                            tmp_file_path = tmp_file.name
                        
                        # åŠ è½½æ–‡æ¡£
                        file_type = uploaded_file.name.split('.')[-1]
                        documents = rag_system.load_document(tmp_file_path, file_type)
                        
                        if documents:
                            # æ·»åŠ æ–‡ä»¶ååˆ°å…ƒæ•°æ®
                            for doc in documents:
                                doc.metadata["filename"] = uploaded_file.name
                            all_documents.extend(documents)
                            st.success(f"âœ… {uploaded_file.name} åŠ è½½æˆåŠŸ")
                        else:
                            st.error(f"âŒ {uploaded_file.name} åŠ è½½å¤±è´¥")
                        
                        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        os.unlink(tmp_file_path)
                    
                    if all_documents:
                        # åˆ‡åˆ†æ–‡æ¡£
                        split_docs = rag_system.split_documents(all_documents)
                        # æ›´æ–°å‘é‡å­˜å‚¨
                        rag_system.create_or_update_vectorstore(split_docs)
                        st.rerun()
        
        st.markdown("---")
        
        # æ¸…é™¤åŠŸèƒ½
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºçŸ¥è¯†åº“", use_container_width=True):
                st.session_state.rag_system.vectorstore = None
                st.success("çŸ¥è¯†åº“å·²æ¸…ç©º")
                st.rerun()
        
        with col2:
            if st.button("ğŸ’­ æ¸…é™¤è®°å¿†", use_container_width=True):
                st.session_state.rag_system.memory.clear()
                st.session_state.messages = []
                st.success("å¯¹è¯è®°å¿†å·²æ¸…é™¤")
                st.rerun()
        
        # ä½¿ç”¨è¯´æ˜
        st.markdown("---")
        st.subheader("ğŸ“– ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        **åŠŸèƒ½ç‰¹ç‚¹ï¼š**
        - ğŸ” æ™ºèƒ½æ£€ç´¢ï¼šä»çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯
        - ğŸ§  è‡ªä¸»å›ç­”ï¼šçŸ¥è¯†åº“æ— ç­”æ¡ˆæ—¶ä½¿ç”¨AIè‡ªèº«çŸ¥è¯†
        - ğŸ’­ ä¸Šä¸‹æ–‡è®°å¿†ï¼šä¿æŒå¯¹è¯è¿è´¯æ€§
        - ğŸ“ å¤šæ ¼å¼æ”¯æŒï¼šTXTã€PDFã€Wordã€CSVç­‰
        - âš¡ å¢é‡æ›´æ–°ï¼šéšæ—¶æ·»åŠ æ–°æ–‡æ¡£
        
        **ä½¿ç”¨æ­¥éª¤ï¼š**
        1. ä¸Šä¼ ç›¸å…³æ–‡æ¡£æ„å»ºçŸ¥è¯†åº“
        2. å¼€å§‹æé—®ï¼Œç³»ç»Ÿä¼šæ™ºèƒ½åŒ¹é…ç­”æ¡ˆ
        3. æ”¯æŒè¿ç»­å¯¹è¯å’Œè¿½é—®
        """)
    
    # ä¸»èŠå¤©åŒºåŸŸ
    st.subheader("ğŸ’¬ æ™ºèƒ½é—®ç­”")
    
    # åˆ›å»ºèŠå¤©å®¹å™¨
    chat_container = st.container(height=500)
    
    # æ˜¾ç¤ºèŠå¤©å†å²
    with chat_container:
        for role, content in st.session_state.messages:
            with st.chat_message(role):
                st.write(content)
    
    # èŠå¤©è¾“å…¥
    if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append(("user", prompt))
        
        with chat_container:
            with st.chat_message("user"):
                st.write(prompt)
            
            with st.chat_message("assistant"):
                # æ„å»ºé—®ç­”é“¾
                qa_chain = rag_system.build_qa_chain()
                
                if qa_chain is not None:
                    # æµå¼è¾“å‡ºå“åº”
                    with st.spinner("æ€è€ƒä¸­..."):
                        try:
                            response = qa_chain.invoke({"question": prompt})
                            st.write(response)
                            
                            # æ·»åŠ åˆ°è®°å¿†å’ŒèŠå¤©å†å²
                            rag_system.add_to_memory(prompt, response)
                            st.session_state.messages.append(("assistant", response))
                            
                        except Exception as e:
                            error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
                            st.error(error_msg)
                            st.session_state.messages.append(("assistant", error_msg))
                else:
                    # æ²¡æœ‰çŸ¥è¯†åº“æ—¶ä½¿ç”¨çº¯LLMå›ç­”
                    no_kb_msg = "çŸ¥è¯†åº“ä¸ºç©ºï¼Œè®©æˆ‘ç”¨è‡ªå·±çš„çŸ¥è¯†æ¥å›ç­”æ‚¨çš„é—®é¢˜ï¼š"
                    st.info(no_kb_msg)
                    
                    try:
                        llm = ChatOpenAI(
                            model_name="gpt-4o", 
                            temperature=0.3, 
                            openai_api_key=os.getenv("OPENAI_API_KEY")
                        )
                        
                        # æ ¼å¼åŒ–å†å²å¯¹è¯
                        history_context = ""
                        if st.session_state.messages:
                            recent_messages = st.session_state.messages[-6:]  # æœ€è¿‘3è½®å¯¹è¯
                            history_context = "\n".join([f"{'ç”¨æˆ·' if role == 'user' else 'åŠ©æ‰‹'}: {content}" for role, content in recent_messages])
                        
                        enhanced_prompt = f"""åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œè¯·å›ç­”ç”¨æˆ·çš„æœ€æ–°é—®é¢˜ï¼š

å¯¹è¯å†å²ï¼š
{history_context}

æœ€æ–°é—®é¢˜ï¼š{prompt}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"""
                        
                        response = llm.invoke(enhanced_prompt).content
                        st.write(response)
                        
                        # æ·»åŠ åˆ°è®°å¿†å’ŒèŠå¤©å†å²
                        rag_system.add_to_memory(prompt, response)
                        st.session_state.messages.append(("assistant", response))
                        
                    except Exception as e:
                        error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}"
                        st.error(error_msg)
                        st.session_state.messages.append(("assistant", error_msg))

    # é¡µé¢åº•éƒ¨ä¿¡æ¯
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("ğŸ’¡ **æç¤º**ï¼šä¸Šä¼ ç›¸å…³æ–‡æ¡£å¯è·å¾—æ›´å‡†ç¡®çš„ç­”æ¡ˆ")
    with col2:
        st.markdown("ğŸ”„ **è®°å¿†**ï¼šç³»ç»Ÿä¼šè®°ä½å¯¹è¯ä¸Šä¸‹æ–‡")
    with col3:
        st.markdown("ğŸ¤– **æ™ºèƒ½**ï¼šæ— ç­”æ¡ˆæ—¶ä½¿ç”¨AIè‡ªèº«çŸ¥è¯†")

if __name__ == "__main__":
    main()
