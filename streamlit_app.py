import streamlit as st
import os
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

st.set_page_config(page_title="åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘", page_icon="ğŸ¦œğŸ”—")

# ---------- 1. ä»æœ¬åœ° Markdown æ–‡ä»¶è·å–æ–‡æ¡£å†…å®¹ ----------
def fetch_document_from_file(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read()
    except FileNotFoundError:
        st.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        st.stop()
    except Exception as e:
        st.error(f"æ— æ³•è¯»å–æ–‡ä»¶: {e}")
        st.stop()
    return ""

# Markdown æ–‡ä»¶è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºå®é™…çš„ Markdown æ–‡ä»¶è·¯å¾„ï¼‰
DOCUMENT_FILE_PATH = "æµ‹è¯•.md"  # ä½¿ç”¨æµ‹è¯•.mdæ–‡ä»¶

def build_retriever():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
        st.stop()

    # ä»æœ¬åœ°æ–‡ä»¶è·å–æ–‡æ¡£å†…å®¹
    raw_docs = fetch_document_from_file(DOCUMENT_FILE_PATH)
    if not raw_docs:
        st.error("æ–‡æ¡£å†…å®¹ä¸ºç©º")
        st.stop()

    # åˆ‡åˆ†é•¿æ–‡æ¡£
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = text_splitter.create_documents([raw_docs])

    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore.as_retriever()

# ---------- 2. æ„å»ºé—®ç­”é“¾ ----------
def get_qa_chain():
    retriever = build_retriever()
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0, openai_api_key=os.getenv("OPENAI_API_KEY"))

    system = (
        "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚\n"
        "è¯·ä½¿ç”¨ä¸‹é¢çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œå¦‚æœä¸çŸ¥é“å°±è¯´â€œæˆ‘ä¸çŸ¥é“â€ã€‚\n\n"
        "{context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system),
        ("human", "{question}")
    ])
    chain = (
        {"context": retriever | (lambda docs: "\n\n".join(d.page_content for d in docs)),
         "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# ---------- 3. Streamlit ä¸»ç•Œé¢ ----------
def main():
    st.markdown("### ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘")

    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chain" not in st.session_state:
        st.session_state.chain = get_qa_chain()

    msgs = st.container(height=550)
    for role, text in st.session_state.messages:
        msgs.chat_message(role).write(text)

    if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜"):
        st.session_state.messages.append(("user", prompt))
        msgs.chat_message("user").write(prompt)

        with msgs.chat_message("assistant"):
            response = st.write_stream(st.session_state.chain.stream(prompt))
        st.session_state.messages.append(("assistant", response))

if __name__ == "__main__":
    main()
